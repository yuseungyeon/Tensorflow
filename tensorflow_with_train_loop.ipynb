{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_with_train_loop.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMoqVL080ls6c74B9kHs86H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7IBux8sHjyQ",
        "outputId": "99a6998a-57e7-42a4-b145-f9392f1a2c48"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLxEbKnIHtXS"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]\n",
        "\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_HYv3ESJDnA"
      },
      "source": [
        "tf.distributed.MirroredStrategy\n",
        "\n",
        "- 모든 변수와 모델 그래프는 장치에 복제됨\n",
        "- 입력이 장치에 고르게 분배\n",
        "- 각 장치는 주어진 입력에 대해 loss, gradient 계산\n",
        "- gradient를 모두 더함으로써 모든 장치들 간에 gradient 동기화\n",
        "- 동기화된 후, 동일한 업데이트가 각 장치에 있는 변수의 복사본에 동일하게 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNc9hginIB1X",
        "outputId": "24764c7d-499f-422e-b1e9-ac86cdbcf539"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV3UYA3dJazt",
        "outputId": "3a4b646b-44a1-4041-9c13-69f525850dc8"
      },
      "source": [
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwiUibQUJl8-"
      },
      "source": [
        "입력 파이프라인 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OidM_tJBJjIh"
      },
      "source": [
        "BUFFER_SIZE = len(train_images)\n",
        "BATCH_SIZE_PER_REPLICA= 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "EPOCHS = 10"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTd--yKNJz1M"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE) \n",
        "\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exObSyU0J7O2"
      },
      "source": [
        "모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKPq45qMJ11I"
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "                               tf.keras.layers.MaxPooling2D(),\n",
        "                               tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "                               tf.keras.layers.MaxPooling2D(),\n",
        "                               tf.keras.layers.Flatten(),\n",
        "                               tf.keras.layers.Dense(64, activation='relu'),\n",
        "                               tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG7NuoPEKS8R"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM4cHbCRKVHr"
      },
      "source": [
        "손실함수 정의\n",
        "\n",
        "1 GPU/CPU가 있는 단일 시스템에서 손실 : 입력 배치의 예제 수로 나뉨\n",
        "\n",
        "4개 GPU, batch_size = 64일 때 : 하나의 입력 배치가 전체 복제본(4개의 GPU)에 걸쳐 분배되며 각 복제본은 크기 16의 입력을 받음\n",
        "각 복제본의 모델은 해당 입력으로 순방향 전달을 수행하고 손실을 계산. 손실은 global_batch_size(64)로 나누어야 함\n",
        "\n",
        "-> 각 복제본에서 gradient가 계산된 후 이를 합산하여 전체 복제본에 걸쳐 동기화가 됨\n",
        "\n",
        "\n",
        "합산할 때는 tf.nn.scale_regularization_loss를 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hYeU0LBKUVE"
      },
      "source": [
        "with strategy.scope():\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True,\n",
        "      reduction=tf.keras.losses.Reduction.NONE)\n",
        "def compute_loss(labels, predictions):\n",
        "  per_example_loss = loss_object(labels, predictions)\n",
        "  return tf.nn.compute_average_loss(per_example_loss, global_batch_size = GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcJIutcWNwIy"
      },
      "source": [
        "# 손실과 정확도를 기록하기 위한 지표 정의\n",
        "\n",
        "with strategy.scope():\n",
        "  test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
        "\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCmKJvjBOQcW"
      },
      "source": [
        "# 훈련 루프\n",
        "\n",
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  checkpoint = tf.train.Checkpoint(optimizer = optimizer, model = model)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrBGPrSeOam4"
      },
      "source": [
        "def train_step(inputs):\n",
        "  images, labels = inputs\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images, training = True)\n",
        "    loss = compute_loss(labels, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_accuracy.update_state(labels, predictions)\n",
        "  return loss\n",
        "\n",
        "def test_step(inputs):\n",
        "  images, labels = inputs\n",
        "\n",
        "  predictions = model(images, training = False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss.update_state(t_loss)\n",
        "  test_accuracy.update_state(labels, predictions)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewqiHqNVOvaG",
        "outputId": "f8cbd628-42c0-47cd-dbea-d9d17a4a81b4"
      },
      "source": [
        "@tf.function\n",
        "def distributed_train_step(dataset_inputs):\n",
        "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis = None)\n",
        "\n",
        "@tf.function\n",
        "def distributed_test_step(dataset_inputs):\n",
        "  return strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "\n",
        "  for x in train_dist_dataset:\n",
        "    total_loss += distributed_train_step(x)\n",
        "    num_batches += 1\n",
        "  train_loss = total_loss / num_batches\n",
        "\n",
        "  for x in test_dist_dataset:\n",
        "    distributed_test_step(x)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      checkpoint.save(checkpoint_prefix)\n",
        "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
        "              \"Test Accuracy: {}\")\n",
        "  print (template.format(epoch+1, train_loss,\n",
        "                         train_accuracy.result()*100, test_loss.result(),\n",
        "                         test_accuracy.result()*100))\n",
        "\n",
        "  test_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3339049220085144, Accuracy: 84.73500061035156, Test Loss: 0.3538040518760681, Test Accuracy: 87.08267211914062\n",
            "Epoch 2, Loss: 0.28882819414138794, Accuracy: 89.43167114257812, Test Loss: 0.3005160987377167, Test Accuracy: 89.27999877929688\n",
            "Epoch 3, Loss: 0.25879883766174316, Accuracy: 90.46333312988281, Test Loss: 0.28906407952308655, Test Accuracy: 89.6300048828125\n",
            "Epoch 4, Loss: 0.23640312254428864, Accuracy: 91.3183364868164, Test Loss: 0.27847766876220703, Test Accuracy: 89.91000366210938\n",
            "Epoch 5, Loss: 0.21524685621261597, Accuracy: 92.04166412353516, Test Loss: 0.2885028123855591, Test Accuracy: 89.3800048828125\n",
            "Epoch 6, Loss: 0.19979551434516907, Accuracy: 92.59832763671875, Test Loss: 0.26059702038764954, Test Accuracy: 90.61000061035156\n",
            "Epoch 7, Loss: 0.1840706169605255, Accuracy: 93.17832946777344, Test Loss: 0.2663477957248688, Test Accuracy: 90.62000274658203\n",
            "Epoch 8, Loss: 0.1705845296382904, Accuracy: 93.61499786376953, Test Loss: 0.26093336939811707, Test Accuracy: 90.90999603271484\n",
            "Epoch 9, Loss: 0.1561310589313507, Accuracy: 94.21833038330078, Test Loss: 0.2646486163139343, Test Accuracy: 90.93000030517578\n",
            "Epoch 10, Loss: 0.14494769275188446, Accuracy: 94.59833526611328, Test Loss: 0.2855350971221924, Test Accuracy: 90.2300033569336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3feeWTqJTO4Z"
      },
      "source": [
        "최신 체크포인트를 불러와 테스트하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpRFwBPLP37_"
      },
      "source": [
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'eval_accuracy')\n",
        "\n",
        "new_model = create_model()\n",
        "new_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ElWEUo5USLJ"
      },
      "source": [
        "@tf.function\n",
        "def eval_step(images, labels):\n",
        "  predictions = new_model(images,training= False)\n",
        "  eval_accuracy(labels, predictions)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W_6fwMnUcXr",
        "outputId": "39b95f1a-4474-4746-a33f-bc4fa492798d"
      },
      "source": [
        "checkpoint = tf.train.Checkpoint(optimizer = new_optimizer, model = new_model)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "for images, labels in test_dataset:\n",
        "  eval_step(images, labels)\n",
        "\n",
        "print('전략을 사용하지 않고, 저장된 모델을 복원한 후의 정확도 : {}'.format(eval_accuracy.result()*100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전략을 사용하지 않고, 저장된 모델을 복원한 후의 정확도 : 90.93000030517578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbgXoqjeU5Ia"
      },
      "source": [
        "데이터셋에 대해 반복작업을 하는 다른 방법들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VftTfnW1Ux2X",
        "outputId": "fda78d7d-3fbb-487f-eaf1-c321ec032588"
      },
      "source": [
        "# iterator 사용\n",
        "\n",
        "for _ in range(EPOCHS):\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  train_iter = iter(train_dist_dataset)\n",
        "\n",
        "  for _ in range(10):\n",
        "    total_loss += distributed_train_step(next(train_iter))\n",
        "    num_batches += 1\n",
        "  average_train_loss = total_loss / num_batches\n",
        "\n",
        "  template = (\"Epoch {}, Loss : {}, Accuracy : {}\")\n",
        "  print(template.format(epoch+1, average_train_loss, train_accuracy.result()*100))\n",
        "  train_accuracy.reset_states()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss : 0.15513095259666443, Accuracy : 94.53125\n",
            "Epoch 10, Loss : 0.1335621476173401, Accuracy : 96.09375\n",
            "Epoch 10, Loss : 0.11737672239542007, Accuracy : 95.625\n",
            "Epoch 10, Loss : 0.12078237533569336, Accuracy : 96.25\n",
            "Epoch 10, Loss : 0.11946958303451538, Accuracy : 96.25\n",
            "Epoch 10, Loss : 0.13709188997745514, Accuracy : 94.53125\n",
            "Epoch 10, Loss : 0.13406801223754883, Accuracy : 95.3125\n",
            "Epoch 10, Loss : 0.1214999109506607, Accuracy : 95.78125\n",
            "Epoch 10, Loss : 0.1460639387369156, Accuracy : 93.90625\n",
            "Epoch 10, Loss : 0.12815073132514954, Accuracy : 95.15625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQaU3m0hVWhe",
        "outputId": "6308e036-a7c7-4b1a-9fed-4c0351a8a403"
      },
      "source": [
        "# tf.function 내부에서 반복하기\n",
        "\n",
        "@tf.function\n",
        "def distributed_train_epoch(dataset):\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  for x in dataset:\n",
        "    per_replica_losses = strategy.run(train_step, args = (x,))\n",
        "    total_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "  num_batches += 1\n",
        "  return total_loss / tf.cast(num_batches, dtype = tf.float32)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = distributed_train_epoch(train_dist_dataset)\n",
        "\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
        "  print (template.format(epoch+1, train_loss, train_accuracy.result()*100))\n",
        "\n",
        "  train_accuracy.reset_states()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
            "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 125.27386474609375, Accuracy: 95.00333404541016\n",
            "Epoch 2, Loss: 116.18745422363281, Accuracy: 95.34833526611328\n",
            "Epoch 3, Loss: 104.72651672363281, Accuracy: 95.82333374023438\n",
            "Epoch 4, Loss: 97.30291748046875, Accuracy: 96.04833221435547\n",
            "Epoch 5, Loss: 92.0727767944336, Accuracy: 96.27833557128906\n",
            "Epoch 6, Loss: 82.01773834228516, Accuracy: 96.74166107177734\n",
            "Epoch 7, Loss: 76.8344955444336, Accuracy: 96.94499969482422\n",
            "Epoch 8, Loss: 71.19281768798828, Accuracy: 97.2066650390625\n",
            "Epoch 9, Loss: 65.19236755371094, Accuracy: 97.413330078125\n",
            "Epoch 10, Loss: 60.38823318481445, Accuracy: 97.61499786376953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDJAU_0CWB-c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}